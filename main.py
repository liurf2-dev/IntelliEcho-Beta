import os
import time
import threading

from langchain import hub
from langchain_core.documents import Document
from typing_extensions import List, TypedDict
from langgraph.graph import START, StateGraph

from dotenv import load_dotenv
from audio_input import get_mp_and_spk_index, clean_up, MicrophoneRecorder, LoopbackRecorder
from audio_recog import AudioRecognition
from kb_build_retrieve import ModelConfig, KnowledgeBaseConfig

load_dotenv()
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = os.getenv("LANG_SMITH_API_KEY")

class State(TypedDict):
    """
    State of the conversation

    microphone: str
        The microphone audio input
    speaker: str
        The speaker audio loopback input
    question: str
        The question asked
    context: List[Document]
        The retrieved context
    answer: str
        The generated answer
        
    """
    microphone: str
    speaker: str

    question: str
    context: List[Document]
    answer: str

class ChatConfig:
    def __init__(self, llm, vec_store):
        self.llm = llm
        self.vector_store = vec_store
        self.ar = AudioRecognition()

    def textualize(self, state: State) -> str:
        # micro_context = self.ar.asr_transcript_by_paraformer("recording/loopback_record.wav")
        max_loopback = max([f for f in os.listdir("recording/") if f.startswith("loopback_record")])
        speaker_context = self.ar.asr_transcript_by_paraformer(f"recording/{max_loopback}")
        return {"speaker": speaker_context}

    def retrieve(self, state: State) -> List[Document]:
        retrieve_prompt = """
        You are an assistant for summary audio input and retrieve the related knowledge context. Use the following pieces of Audio and Question input to retrieve context. If you don't find any related context, just say that you don't know. Keep the answer concise.
        Speak Audio: {speaker}
        Question: {question}
        Answer:
        """
        context_kb = self.vector_store.similarity_search(state["speaker"])
        return {"context": context_kb}

    def generate(self, state: State) -> str:
        docs_content = "\n\n".join(doc.page_content for doc in state["context"])
        prompt = hub.pull("rlm/rag-prompt")
        messages = prompt.invoke({"question": state["question"], "context": docs_content})
        response = self.llm.invoke(messages)
        return {"answer": response.content}


def record_audio_thread(mp_idx, lb_idx, time_interval):
    """å½•éŸ³çº¿ç¨‹å‡½æ•°"""
    global is_running
    
    print("ğŸ™ï¸ å½•éŸ³å·²å¼€å§‹...")
    # åˆå§‹åŒ–å½•éŸ³å™¨
    # micro_rec = MicrophoneRecorder(mp_idx)
    loop_rec = LoopbackRecorder(lb_index=lb_idx, time_interval=time_interval)
    
    # å¼€å§‹å½•éŸ³
    # micro_rec.start()
    
    loop_rec.start()
    
    # æŒç»­å½•éŸ³ç›´åˆ°åœæ­¢ä¿¡å·
    while is_running:
        time.sleep(0.5)
    
    # åœæ­¢å½•éŸ³ï¼Œä¿å­˜æ–‡ä»¶
    # micro_rec.stop()
    loop_rec.stop()
    print("ğŸ™ï¸ å½•éŸ³å·²åœæ­¢")


def analyze_audio_thread(embeddings, chroma_ef, llms, time_interval):
    """éŸ³é¢‘åˆ†æçº¿ç¨‹å‡½æ•°"""
    global is_running
    
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    kb = KnowledgeBaseConfig(chroma_ef, embeddings)
    vec_store = kb.get_or_create_kb("kb_files/ä¼˜åŠ¿è°ˆåˆ¤ï¼šé€‚ç”¨äºä»»ä½•åœºæ™¯çš„ç»å…¸è°ˆåˆ¤.pdf", doc_size=1000, doc_overlap=100)
    
    # åˆå§‹åŒ–èŠå¤©é…ç½®
    kb_chat = ChatConfig(llms, vec_store)
    
    while is_running:
        # ç­‰å¾…æŒ‡å®šæ—¶é—´é—´éš”
        time.sleep(time_interval+3)
        
        if not is_running:
            break
            
        print("\nğŸ§  åˆ†æçº¿ç¨‹å·²å¯åŠ¨...")
        
        # æ„å»ºå¹¶æ‰§è¡Œå›¾
        graph_builder = StateGraph(State).add_sequence([kb_chat.textualize, kb_chat.retrieve, kb_chat.generate])
        graph_builder.add_edge(START, "textualize")
        graph = graph_builder.compile()

        for step in graph.stream(
                {"question": "Please show me the meeting key points"}, stream_mode="updates"
        ):
            print(f"{step}\n\n----------------\n")


def input_listener_thread():
    """è¾“å…¥ç›‘å¬çº¿ç¨‹å‡½æ•°"""
    global is_running
    print("\nğŸ›‘ è¾“å…¥ 'stop' å¹¶æŒ‰å›è½¦ç»ˆæ­¢ç¨‹åº...")
    while True:
        user_input = input().strip().lower()
        if user_input == 'stop':
            is_running = False
            print("â³ æ­£åœ¨åœæ­¢ç¨‹åº...")
            break


if __name__ == "__main__":
    ###============== åˆå§‹åŒ–åŒº ==============###
    all_model_config = ModelConfig(api_key=os.getenv("QWEN_API_KEY"), base_url=os.getenv("QWEN_BASE_URL"))
    embeddings = all_model_config.embedding_model()
    chroma_ef = all_model_config.chroma_ef()
    llms = all_model_config.llm_model()

    mp_idx, lb_idx = get_mp_and_spk_index()
    time_interval = 60 # æ¯æ¬¡å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰

    is_running = True
    
    ###============== åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹ ==============###
    # åˆ›å»ºçº¿ç¨‹
    record_thread = threading.Thread(target=record_audio_thread, args=(mp_idx, lb_idx, time_interval))
    analyze_thread = threading.Thread(target=analyze_audio_thread, args=(embeddings, chroma_ef, llms, time_interval))
    input_thread = threading.Thread(target=input_listener_thread)
    
    # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œè¿™æ ·ä¸»ç¨‹åºé€€å‡ºæ—¶å®ƒä»¬ä¹Ÿä¼šé€€å‡º
    record_thread.daemon = True
    analyze_thread.daemon = True
    
    # å¯åŠ¨çº¿ç¨‹
    record_thread.start()
    analyze_thread.start()
    input_thread.start()
    
    # ç­‰å¾…è¾“å…¥çº¿ç¨‹å®Œæˆï¼ˆå³ç”¨æˆ·è¾“å…¥stopï¼‰
    input_thread.join()
    
    # ç­‰å¾…å…¶ä»–çº¿ç¨‹å®Œæˆ
    print("æ­£åœ¨ç­‰å¾…çº¿ç¨‹ç»“æŸ...")
    time.sleep(2)  # ç»™å…¶ä»–çº¿ç¨‹ä¸€äº›æ—¶é—´æ¥ä¼˜é›…åœ°ç»“æŸ
    
    ###============== æ¸…ç†ç¼“å­˜åŒº ==============###
    clean_up()
    print("ç¨‹åºå·²ç»“æŸ")